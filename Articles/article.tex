\documentclass[english,10pt,twocolumn]{IEEEtran}
\usepackage[utf8]{inputenc}  

\usepackage[english]{babel} 
\usepackage[T1]{fontenc}
%\usepackage{fullpage}
\usepackage[]{bbm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage{pstricks}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage[autolanguage]{numprint}
\usepackage[affil-it]{authblk}
%\usepackage[squaren,Gray]{SIunits}
%\usepackage{tikz}
%
%\usepackage[style=numeric,backend=bibtex]{biblatex}
%\pagestyle{myheadings}


\title{Storing messages with multipartite neural cliques}
\author[]{Nissim Zerbib}
\affil{Département d'Informatique, École normale supérieure, Paris, France}

\author[]{Vincent Gripon}
\affil{Département d'Électronique, Télécom Bretagne, Brest, France}

\author{?}

\date{
	}
%

\renewcommand{\le}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\Re}{\textrm{Re}}
\renewcommand{\ge}{\geqslant}
%\newcommand{\sgn}{\textrm{le signe de }}
\newcommand{\E}{\mathcal{E}}
\newcommand{\V}{\vect{\mathcal{V}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\z}[1]{\Z/#1\Z}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathbb{U}}
\newcommand{\f}{\widehat{f}}
\newcommand{\e}[1]{e^{-2 \pi i #1}}
\newcommand{\ex}[1]{e^{2 \pi i #1}}
\renewcommand{\L}[1]{L^{#1} (\R )}
\newcommand{\leg}[2]{\left(\dfrac{#1}{#2}\right)}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\vect}[1]{\overrightarrow{#1}}
\newcommand{\disp}[1]{\displaystyle{#1}}

\setcounter{secnumdepth}{3} %pour la numérotation subsubsection
\setcounter{tocdepth}{3} %pour la numérotation dans la table de matières
\renewcommand{\theenumi}{\roman{enumi})}
%\renewcommand{\thepart}{\Alph{part}}
%\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\arabic{subsection}}
\renewcommand{\thesubsubsection}{\alph{subsubsection}}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\newcommand{\fonction}[5]{\begin{array}{cllll}
#1 & : & #2 & \longrightarrow & #3 \\
&    & #4 & \longmapsto & #5 \end{array}}
\newcommand{\fonc}[3]{\begin{array}{lllll}
#1: & #2 & \mapsto & \displaystyle{#3}\end{array}}
\newcommand{\app}[3]{
	#1 : #2 \mapsto \displaystyle{#3}}
\newcommand{\fonct}[3]{\begin{array}{ccccc}
#1: & #2 & \longrightarrow & \displaystyle{#3}\end{array}}
\newcommand{\pent}[1]{\lfloor #1 \rfloor}

% Le point-virgule bien espacé
\newcommand{\pv}{\ensuremath{\, ; }}
% Les intervalles
   % fermé - fermé
   \newcommand{\interff}[2]{\ensuremath{\left[ #1 \pv #2 \right]}}
   % fermé - ouvert
   \newcommand{\interfo}[2]{\ensuremath{\left[ #1 \pv #2 \right[}}
   % ouvert - fermé
   \newcommand{\interof}[2]{\ensuremath{\left] #1 \pv #2 \right]}}
   % ouvert - ouvert
   \newcommand{\interoo}[2]{\ensuremath{\left] #1 \pv #2 \right[}}


\theoremstyle{definition}
\newtheorem{theoreme}{Theorem}
\renewcommand{\thetheoreme}{\arabic{section}.\arabic{theoreme}}
%\providecommand{\keywords}[1]{\textbf{Index terms---} #1}

\newcommand{\comp}[3]{\left\{
	#1\,;\, #1\in #2 \, /\, #3
	\right\}}
\newcommand{\param}[2]{\left\{
	#1\, /\, #2
	\right\}}


\begin{document}

	\maketitle

	 \begin{abstract}
	 	We extend recently introduced associative memories based on clustered cliques to multipartite cliques. We propose a variant of the classic retrieving rule. We study its performance relatively to the former one for retrieving partially erased or corrupted messages. We provide both analytical and simulation results showing improvements in both networks efficiencies and resilience to damages. We compute asymptotic capacities of these networks.
	 \end{abstract}
	 
	%\keywords{associative memory, error correcting code, cliques, multipartite cliques, neural networks}
	
	\begin{IEEEkeywords}
	associative memory, error correcting code, cliques, multipartite cliques, neural networks
	\end{IEEEkeywords}
	\section{Introduction}
	
		
		Associative memories
		
		Recently, a new type of associative memories was proposed by Gripon and Berrou \cite{GriBer20117}.
		
		GBNN can also be used to retrieve messages from erroneous versions of them. We derive a formula for error rate in case of errors on messages.
		
		Fault tolerance of those networks was extensively studied in \cite{LedGriRabGro20145}. We extend this to multipartite cliques and show significant improvements in resilience to faults.
	
	
	\section{Networks of neural cliques}
	
	An associative memory is able to store messages and retrieve them through a partial versions of them.

	We first describe how messages can be stored in a binary clustered network of multipartite cliques. Secondly we detail how messages can be retrieved after being stored in such a network.
	
	
	\subsection{Storing messages}
	
	Our goal is to store messages so as to be able to retrieve them from partial or erroneous versions of them.
	
	We will consider a set $\mathcal{M}$ of messages of fixed length $c$ over a finite alphabet $\mathcal{A}$.
	
	Messages in $\mathcal{M}$ are stored in a clustered neural network, of $c$ clusters of $l$ units each. Units can take two values $0$ (deactivated) and $1$ (activated). The network is therefore binary. A cluster will correspond to a symbol in a message and the pattern of unit activations in a cluster will determine which symbol in the alphabet the cluster represents. 
	
	A binary constant weight code of weight $a$ is a code where all codewords have the same weight $a$, i.e. the same number of $1$ in them. Previous works (\cite{GriBer20114, GriBer20122}) considered the storage of binary constant weight codewords of weight $1$. We study the case of arbitrary weight. It is convenient to take $\mathcal{A}$ as the set of all binary constant weight codewords of length $l$ and of weight $a$, where $a$ is a positive integer parameter which we will call number of activities and which satisfies $a\le l$. We now have ${l \choose a} = |\mathcal{A}|$. We set $m = | \mathcal{M} |$.
	
	%Let $\mathcal{M}$ be a set of messages of length $c$ over a finite alphabet $\mathcal{A} = \{1, 2, \hdots, {l \choose a}\}$ (where $a$ is a positive integer) and 
	 
	
	%binary constant weight codes of weight $a$.
	
	States of the network are characterized by which units are activated. A message corresponds to one, and only one, state of the network. A message $y$ of length $c$ can be seen as an array $x$ of length $c\dot l$ where for each $j\in [1,c]$, the $j$-th symbol of $y$ denoted $y_j$ is represented by $x[(j-1)l+1\hdots jl]$.\footnote{This is a clustered representation. $x$ is separated in $c$ clusters of $l$ units.} To sum up the $k$-th unit in the $j$-th cluster corresponds to the index $l = (j-1)l+k$. We say the network is in state $x$ if for all $j\in [1, c]$ and $k\in [1,l]$, units $k$ in cluster $j$ of the network is active if, and only if, $x[(j-1)l+l] = 1$. 
	
	%Each message $x = (x_j)_{1 \le j \le c}$ is mapped to $y = (y_j)_{1 \le j \le c}$ where $y_j$ is the binary representation of the $x_i$-th $a$-combination of $l$ elements (the mapping doesn't matter as long as it is fixed).\footnote{This is a local code of constant-weight $a$.}
	
	Storing a message $y$ in the network amounts to consider the network in state $x\,$%, that is for all $j$ and $k$ such as $1 \le k \le l$ and $1 \le j \le c$, the unit $k$ in the cluster $j$ is activated if and only if $x[(j-1)l+k] = 1$ 
	; then adding all edges between activated units to the network excepting for units sharing the same cluster. Note that edges already present in the network remain unchanged. Thus we obtain a multipartite clique between activated units representing a message.% The parameter $a$ describing the number of activated units per cluster will be called number of activities.
	
	
	
	%The state of the network corresponds to a message.
	
	We can describe the network which is an undirected unweighted graph, by its adjacency matrix $W$. For a message $x$, $x x^{\intercal}$ is the matrix describing the graph where all edges between active units exist, and only them. Therefore we can express $W$ in function of the following :
	
	$\tilde{W} = \max_{x\in \mathcal{M}} x x^{\intercal}$ which is the matrix where all edges between units activated for each state $x$ in the same cluster have been added.
	$W$ can now be expressed as :\vspace*{1mm}
	
	$W_{ll'} = \bigg \{
  \begin{tabular}{ccc}
  $0$ & if $\exists j\in [1, c], \, (j-1)c+1 \le l,l' \le jc$ \\
  $\tilde{W}_{ll'}$ & otherwise 
  \end{tabular}
  $
  \vspace*{1mm}
  
  We obtain a binary symmetric matrix $W$ with null blocks on the diagonal.
  
  As taking a maximum is associative and commutative, storing can be done online and is independent of the order in which messages are stored.
		
	\subsection{Retrieving messages}	
		
		Messages are retrieved by iterative application of a correcting rule. We adapt the rule first developed in \cite{GriBer20117} to multipartite cliques.
		
%		\subsubsection{The "winner takes all" rule}
%		
%		take only maximum score
%	
%		\subsubsection{The "$w$-winners take all" rule}
		

		We make a small modification the classic retrieving rule to fully exploit the local regularity of the code (constant weight of $a$). We keep activated all units that achieve a score equal or greater than the $w$-th score. (where $w$ is a positive integer parameter)
		
		%$a$-rank
		
		%Although it is a less biologically plausible rule, it has the same algorithmic complexity, since selecting the $w$-th greater element in an array of $n$ elements is $O(n)$ in complexity.
		
		Intuitively the choice of $w = a$ is optimal (it it is the most natural one with $w = 1$ as well).
		%Algorithm \ref{asumsum} is a description of the algorithm\footnote{No optimizations are described here (early stopping etc.)} :
		
		
		We consider an altered version $\tilde{x}$ of a message $x$ which we are trying to retrieve. We set $\tilde{x}^0 = \tilde{x}$.
		
		At each step $t$, we compute the scores, that is to mean the number of active units each unit is connected to, plus a memory effect $\gamma $ accounting for units begin already active. The array of scores at step $t$ is :  $s_{t} = \gamma \tilde{x_{t}} + W \tilde{x_{t}}$
		
		To detail the final step of the retrieving rule, we introduce a function $\mbox{select}$ such as for an array $v$, $\mbox{select}(w, v)$ is equal to the $w$-th greatest element of $v$ (counting repetitions).\footnote{$\mbox{select}(3, [3,2,1,2,0]) = 2$}
		
		\begin{align*}
		%$
		&\tilde{x}_{t+1}[(j-1)c+k] =\\ \bigg \{
  			&\begin{tabular}{ccc}
  			$1$ & if $s_t[(j-1)c+k] \ge  \mbox{select}(w, s_t[(j-1)l+1 \hdots jl)$ \\
  			$0$ & otherwise 
  		\end{tabular}
  		%$
  		\end{align*}
		
		We can for example fix a maximum number of steps to the algorithm or even stop at one point if $\tilde{x}_{t+1} = \tilde{x}_t$. The number of iterations can of course improve the retrieval rate.
		
		
		
%		\begin{algorithm}[!htb]
%		\caption{$w$-sum of sum \label{asumsum}}
%		\begin{algorithmic}[1]
%		\Require{$v$ a binary array of length $n = c\cdot l$, $W$ the binary weight matrix representing the network, $S$ the number of iterations, $\gamma$ memory effect}
%		\Procedure{Sum of sum}{$v, W, \gamma, S, w$}
%		%\Statex
%		\For{$s$ from $1$ to $S$}
%		\State $s \gets \gamma v + W v$
%		\For{$i$ from $1$ to $c$}
%			\Let{treshold}{Select(s[$i$], $w$)} \Comment{Select(t, w) returns the element that would be in rank $w$ if the array t was sorted}
%			\For{$j$ from $1$ to $l$}
%			\Let{$v[i][j]$}{$s[i][j]$ $\ge$ treshold}
%			\EndFor
%		\EndFor
%		\EndFor
%		%\State\Return v
%		\EndProcedure
%		\end{algorithmic}
%		\end{algorithm}
	
	
	We now study the performance of the new retrieving rule on distinct problems of retrieval : first partially erased messages, then erroneous messages.%, finally partially erased messages when the network has been damaged.
	
	\section{Retrieval performance}	
	
	Messages will be considered as independent and identically distributed according to the uniform distribution. We compute theoretical error rates after one iteration of the retrieving rule. We compare those results to empirical error rates obtained through simulations. We also compare efficiencies of distinct networks for distinct values of the parameter $a$ (weight of the constant weight code) in the sense of information theory and show improvements over the case where $a = 1$.

	\subsection{Retrieving partially erased messages}		
		
	\subsubsection{Analytical result}
	
	We first compute the theoretical density of the network, that is to mean the probability for a random edge $(l, l')$ to be present in the network (i.e. $W_{ll'} = 1$). We then express the theoretical error rate after one iteration of the retrieving rule in function of the density. All this computations are done assuming connections between units are independent, which is false but is a convenient approximation that gives results very close from empirical results.
	
	%number of neurons per cluster $l$
	
	%number of clusters $c$
	
	%number of erased clusters $c_e$
	
	%number of messages : $m$
	
	%number of activities per cluster : $a$
	
	%density : $d$
	
	%edge : $(i, j)$
	
	%We generalise results on erasures obtained in \cite{GriBer20117} to multipartie cliques.
	
	Erasures of a symbol in a message corresponds to an erased cluster in the network. An erased cluster is a cluster of units which are all inactive, that is to mean set to $0$.
	
	Suppose we are trying to retrieve a message where $c_e$ clusters have been erased. %(i.e. all units in those clusters are set to $0$).
	
	As the probability for $l$ and $l'$ to be active in their respective clusters for one message is $\frac{a}{l}$ providing messages are identically distributed, it follows that the probability for the edge $(l,l')$ of not being added in the network for one message is $1 - \left(a/l\right)^2$. Since messages are independent, the density $d$, which is the probability for an edge to be in the network, can be expressed as :
	
	\begin{align}
		\label{formula_density}
		d = 1 - \left( 1 - \left(\frac{a}{l}\right)^2 \right)^m 
	\end{align}		
	
	Thanks to the memory effect $\gamma$, activated units in a non-erased cluster stay activated and are the only ones doing so in their cluster. For units in erased clusters, the maximum attainable score is $a(c - c_e)$. Units corresponding to the original message achieve this score. Besides the probability for a unit $v$ to achieve the maximum score in such a cluster, that is to mean $a(c-c_e)$ is $d^{a(c-c_e)}$.
	
	%probability for a vertex $v$	not to achieve the maximum : $1 - d^	{a(c-c_e)}$
	
	Since unit activations are independent (as messages are themselves independent), the probability for none of the vertices of one erased cluster, excepting the correct ones, to achieve the maximum is $\left(1 - d^	{a(c-c_e)}\right)^{l-a}$.
	
	Scores in clusters being also independent, the probability for none of the vertices in any erased cluster, excepting the correct ones, to achieve the maximum in their respective clusters is $\left(1 - d^	{a(c-c_e)}\right)^{c_e(l-a)}$.
	
	Whence error rate in retrieving messages is : 
	\begin{align}	
	P_{err} = 1 -	\left(1 - d^	{a(c-c_e)}\right)^{c_e(l-a)} 
	\end{align}	 
	
	In order to compare different networks using different alphabets, we define the efficiency of the network relatively to the hardness of the problem (how much it is efficient compared to a perfect associative memory). A perfect associative memory would use exactly the same number of bits than the original messages and retrieve messages by maximum likelihood (ambiguities causing errors). Efficiency is the amount of information retrieved divided by the hardness of the problem.
	
	The network is a binary symmetric matrix with null diagonal and can therefore be stored in memory as $\frac{c(c-1) l^2}{2}$ bits. Since messages are not stored in an ordered fashion, the information stored is the set of messages, thereby it amounts to $\log_2(\frac{({l \choose a}^c)^m}{m!}) \mathop{=}_{m \rightarrow +\infty} m(c \log_2{l \choose a } - \log_2(m) + \frac{1}{\log 2} + o(1)) $ (thanks to  Stirling's formula). The information effectively stored in the network is $P_{retrieve} \times  \frac{2m \left(c \log_2{l \choose a } - \log_2(m) + \frac{1}{\log 2} \right)}{c(c-1)l^2}$.
	
	Ambiguities would occur in a perfect associative memory if at least two messages were identical over non-erased letters. The probability it doesn't happen is $ (1-{l \choose a}^{c - c_e})^{m-1}$.
	
	
	$\eta_m = P_{retrieve}  \frac{2 m\left(c \log_2{l \choose a } - \log_2(m) + \frac{1}{\log 2} \right)}{c(c-1)l^2} \times (1-(\frac{1}{{l \choose a}})^{c - c_e})^{-(m-1)}$
	%\frac{c}{(c-c_e) \log_2{l \choose a }}
	
	The maximum information that can be stored in the network is therefore $\eta = \max_x \eta_x $, which we will define as the efficiency of the network.
	
	\subsubsection{Simulations}
		Theoretical and empirical densities match very closely, see Figure \ref{densiteth}. %gap of theoretical density with density in simulations : under $1$ percent, 
		
		
		
		Simulations agree with the analytical results for error rate, see Figure \ref{erasuresth}. We observe a small shift.
	

	For one iteration (for erasures, not for errors), "winner takes all" is the same as "$a$-winners take all".
	
	For multiple iterations $a$-winners take all is a significant improvement.
	%The variant of the rule significantly improves the retrieval rate over multiple iterations. (For one iteration, is is the same as taking only maximum scores.)
	
	%See Figure \ref{erasuresth}	
	
		
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/remplacement_densite_c8l256a4}
		\caption{Theoretical and empirical densities for a network of 8 clusters, 256 units per cluster and 4 activated units per cluster}
			\label{densiteth}
	\end{figure}
	
	
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/remplacement_figure2g1} %{comparaison_regles_pool5_a2c4l512e2}
		\caption{Evolution of error rate for increasing number of stored messages, 2 activities per cluster, 4 clusters, 512 units per cluster, two erasures, each point is the mean of 5 networks with 1000 sampled messages, analytical result (for one iteration) in continuous black}
			\label{erasuresth}
		\end{figure}
		
		Figure \ref{comperth} shows improvements in capacities over unipartite cliques network. The more the problem is difficult, the more redundancies introduced by multipartite cliques are useful.
		
		\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/5portant_erasures_c8l256}
		\caption{Comparison between different number of activities per cluster :  8 clusters, 256 units per cluster, 1000 sampled messages, pools of 5}
		\label{comperth}
	\end{figure}
%	
%	\newpage
%	$\,$
%	\newpage
%	$\,$
%	\newpage
	
	\subsection{Retrieving corrupted messages}
	
	\subsubsection{Analytical result}
%	Formula for errors instead of erasures : 
%	
%	gc : good (à remplacer par right/wrong ?) unit in correct cluster
%	
%	ge : good unit in erroneous cluster
%	
%	abe : bad activated unit in erroneous cluster
%	
%	be : others bad (à remplacer par wrong ou incorrect etc.) units in erroneous cluster
%	
%	bc : bad unit in correct cluster
	
	We provide a formula for error rate in case of messages with corrupted clusters.

	Messages can be corrupted, our models for errors is : given $c_e$ the number of erroneous clusters, we draw at random $c_e$ clusters and in each of these draw at random, uniformly, a new letter in the alphabet that will replace the old one (it could be the same).
	
	Knowing the density of the network, which follows formula (\ref{formula_density}), we can compute the laws of probabilities for scores achieved by different type of units in different clusters. We can divide units in 5 groups. Subscript $g$ will indicate a unit being part of the message (it should be activated), subscript $b$ the contrary (the unit should not be activated). Subscript $a$ means the unit is already activated. Subscript $c$ corresponds to a correct cluster, $e$ to an erroneous cluster.
	Correct units in uncorrupted clusters achieve at least a score of $a(c-c_e - 1) + \gamma$. 
	Being connected to a given activated unit in a corrupted cluster occurs with probability $d$, since errors are identically distributed. Moreover corrects units see their scores shift, as do all activated units respectively thanks to being part of the original message and the memory effect $\gamma$. We can express those probability laws as : 
	For example an \textbf{a}ctivated \textbf{g}ood unit in a \textbf{c}orrect cluster starts with a base score of $ a(c - c_e - 1) + \gamma$. The probability it has to be connected to a random activated unit in an erroneous cluster is $d$. Thereby its law is :
	
	%$P(n_{agc} = (c - c_e - 1) + \gamma + x) = {c_e \choose x} d^x (1-d)^{ce-x}$
	$P(n_{agc} = a(c - c_e - 1) + \gamma + x) = {a c_e \choose x} d^x (1-d)^{a ce-x}$
	
	Correct units in corrupted clusters achieve at least a score of $a(c - c_e)$. 
	$P(n_{ge} = a(c - c_e) + x) = {a (c_e - 1) \choose x} d^x (1-d)^{a (c_e - 1)-x}$
	
	%$P(n_{ge} = (c - c_e) + x) = {c_e - 1 \choose x} d^x (1-d)^{c_e-1-x}$
	
	Activated units benefits of the memory effect $\gamma$ :	\\
	$P(n_{abe} = \gamma + x) = {a(c - 1) \choose x} d^x (1-d)^{a(c-1)-x}$
	%$P(n_{abe} = \gamma + x) = {c - 1 \choose x} d^x (1-d)^{c-1-x}$
	
	
	%$P(n_{be} = x) = P(n_{bc} = x) = {c - 1 \choose x} d^x (1-d)^{c- 1 -x}$
	Finally $P(n_{be} = x) = P(n_{bc} = x) = {a(c - 1) \choose x} d^x (1-d)^{a(c- 1) -x}$	
	
	A message is retrieved after one iteration, if and only if correct units achieve strictly the best scores.	
	
%	\begin{align}
%	P_{retrieve} = \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{agc} = n) \left [ \sum_{x=0}^{n-1} P(n_{bc} = x) \right]^{l-1} \right ]^{c - c_e} \\ \times \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{ge} = n) \left [ \sum_{x=0}^{n-1} P(n_{bc} = x) \right]^{l-2} \left [ \sum_{x=0}^{n-1} P(n_{abc} = x) \right] \right ]^{c_e}
%	\end{align}
	
%	\begin{align*}
%	P_{retrieve} = \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{agc} = n)  P(n_{bc} < n)^{l-1} \right ]^{c - c_e} \\ \times \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{ge} = n) P(n_{bc} < n)^{l-2} P(n_{abc} < n) \right ]^{c_e}
%	\end{align*}

	
	%à changer : prendre en compte les $a$ unites activés : problème des recouvrements !! on est trop optimiste si grand nombre d'activités (ou pessimiste ?)
	
	
	
	The probability for all good units of a correct cluster (resp. of an erroneous cluster) to achieve a score equal or greater than $x$, with at least one having a score of $x$ is $P_{ac} (x) = \sum_{k = 1}^{a} { a \choose k }   P(n_{agc} = x)^k P(n_{agc} > x)^{a-k}$ (resp. $P_e(x) = \sum_{k = 1}^a { a \choose k } P(n_{ge} = x)^k P(n_{ge} > x)^{a-k}$)

	
	
%	\begin{align*}
%	&P_{retrieve} = \left [ \sum_{n = 1}^{a(c-1) + \gamma} \left [ \sum_{k = 1}^{a} { a \choose k }   P(n_{agc} = n)^k P(n_{agc} > n)^{a-k} \right ] P(n_{bc} < n)^{l-a} \right ]^{c - c_e}\\ 
%	 &\times \left [ \sum_{n = 1}^{a(c-1) +\gamma} \left[ \sum_{k = 1}^a { a \choose k } P(n_{ge} = n)^k P(n_{ge} > n)^{a-k} \right ] P(n_{be} < n)^{l-2a} P(n_{abe} < n)^a \right ]^{c_e} \numberthis \label{from_err_th}
%	\end{align*}
		\begin{align*}
	&P_{retrieve} = \left [ \sum_{x = 1}^{a(c-1) + \gamma} P_{ac}(x) P(n_{bc} < x)^{l-a} \right ]^{c - c_e}\\ 
	 &\times \left [ \sum_{x = 1}^{a(c-1) +\gamma} P_e(x) P(n_{be} < x)^{l-2a} P(n_{abe} < x)^a \right ]^{c_e} \numberthis \label{from_err_th}
	\end{align*}
	
	with the approximation that there is few activated units that should be kept active in an erronenous cluster.

%	To be really precise : (ne pas oublier de tenir compte changement des lois, formule inexacte pour l'instant)
%	\begin{align*}
%	&P_{retrieve} = \left [ \sum_{n = 1}^{a(c-1) + \gamma} \left [ \sum_{k = 1}^{a} { a \choose k }   P(n_{gc} = n)^k P(n_{gc} > n)^{a-k} \right ] P(n_{bc} < n)^{l-a} \right ]^{c - c_e}\\ 
%	 &\times \left [ \sum_{y = 0}^a \left( \frac{{l-y \choose a} - \sum_{z = y+1}^{a} {l-z \choose a }}{{l \choose a}}\right)\sum_{n = 1}^{a(c-1) +\gamma} \left[ \sum_{k = 1}^a { a \choose k } P(n_{ge} = n)^k P(n_{ge} > n)^{a-k} \right ] P(n_{bc} < n)^{l-a - y} P(n_{abc} < n)^{a - y} \right ]^{c_e}
%	\end{align*}	
	
	$P_{err} = 1 - P_{retrieve}$	

	
	
	\subsubsection{Simulations}
	See Figure \ref{corruptth} compares the two rules.
	"$a$-winners take all" better than "winner takes all" with errors instead of erasures, even for one iteration.
	
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/remplacement_figure3g1}
		\caption{2 activities per cluster, 4 clusters, 512 units per cluster, one erroneous cluster, each point is the mean of 5 networks with 1000 sampled messages, analytical result in continuous black}
		\label{corruptth}
	\end{figure}
		
		$\eta_m = P_{retrieve}  \frac{2 m\left(c \log_2{l \choose a } - \log_2(m) + \frac{1}{\log 2} \right)}{c(c-1)l^2} \times \left [\sum_{k = c_e+1}^{c} \left (1-\frac{1}{{l \choose a}} \right )^k \left (\frac{1}{{l \choose a}} \right )^{c-k} \right ] ^{-(m-1)}$
		
		

	
	\section{Resilience}
	\subsection{Analytical result}
	%Spéculations en Français 
	%Peut-être un intérêt pour cluter based associative memories build from unreliable storage : si une arête porte moins d'info, on peut peut-être en supprimer plus (mais rajout ?!)
	
	As information on a message is shared across connections with multiple activated units per cluster, such networks are more resilient to damages (for one message information is carried by $a^2 {c \choose 2}$ edges. $v_c$ will indicate a correct unit that should be activated, $v$ the other type of units.
	
	Our deviation model is the same as \cite{LedGriRabGro20145}\footnote{Compared to this article, we do not draw at random a letter in case of ambiguity, which hurts performance a bit.}, that is random binary noise on edges. We generalise results of \cite{LedGriRabGro20145} to multipartite cliques.
	
	%gamma = infini meilleur sur une itération
	We assume $\gamma = \infty$ for one iteration as it prevents activated neurons to be deactivated.
	With probability $1 - \psi$, one correct vertex is still connected to a given activated unit in an intact cluster. Thus $P(n_{v_c} = x ) = {a (c_k-1) \choose x} (1-\psi)^{x} \psi ^ { a (c - c_e -1) - x }$	
	
	%$P(n_{v_c} = x) = {a c_k \choose x} (1-\psi)^{x} \psi ^ { a (c - c_e) - x }$
	
	The probability for an edge to be present in the network after damages is : $P_+ = \psi (1 - d) + (1 - \psi) d$. So for other units $P(n_v = x) = {a (c - c_e) \choose x} P_+^x (1-P_+)^{a (c - c_e) -x }$.
	
	% À corriger avec sommes multiparties
%	$P(\mbox{no other vertex activated in this cluster})= \sum_{n_0 = 1}^{a (c - c_e)} P(n_{v_c} \ge n_0)^a \left [ \sum_{x = 0}^{n_0 - 1} P(n_v = x) \right]^{l-a}$
%	
	% Approx : ?! $P(\mbox{no other vertex activated in any cluster})=  \left ( \sum_{n_0 = 1}^{a ( c - c_e)} P(n_{v_c} \ge n_0)^a \left [ \sum_{x = 0}^{n_0 - 1} P(n_v = x) \right]^{l-a } \right)^{c_e}$
	
	%Sans tenir compte du choix au hasard si plus sont activés. Pour une itération $\gamma = \infty$. Pour plusieurs $\gamma = 1$.
	
	The probability for all correct units to achieve a score equal or greater than $x$, with at least one achieving a score of $x$ is $P_c (x) =\sum_{k = 1}^{a} { a \choose k }   P(n_{v_c} = x)^k P(n_{v_c} > x)^{a-k} $

	%If $\gamma$ is big enough,
	It follows that $P(\mbox{no other vertex activated in one cluster})= \sum_{x = 1}^{a (c - c_e)} P_c(x) P(n_v < x)^{l-a}$.%[ \sum_{x = 0}^{n - 1} P(n_v = x) \right]^{l-a }$
	
	%P(\mbox{no other vertex activated in any cluster})
%	\begin{align*}
%	P_{retrieve}=  \Bigg ( \sum_{n = 1}^{a ( c- c_e)} \left [ \sum_{k = 1}^{a} { a \choose k }   P(n_{v_c} = n)^k P(n_{v_c} > n)^{a-k} \right ]\\
%	\left [ \sum_{x = 0}^{n - 1} P(n_v = x) \right]^{l-a } \Bigg)^{c_e}	
%	\end{align*}

	\begin{align}
	P_{retrieve}=  \left( \sum_{x = 1}^{a ( c- c_e)} P_c(x) P(n_v <x)^{l-a} \right )^{c_e}
	\label{psi_formula_th}
	\end{align}

	\subsection{Simulations}
	
	With random noise on edges, hardness of the problem must take into account the capacity of the binary symmetric channel (the closer $\psi$ is of $0.5$, the harder the problem is). Our new is efficiency is written as :  	
	$\eta_{m, \psi} = P_{retrieve} m  \frac{2 \left(c \log_2{l \choose a } - \log_2(m) + \frac{1}{\log 2} \right)}{c(c-1)l^2} \times (1-{l \choose a}^{c - c_e})^{-(m-1)} \times \frac{1}{1 + \psi \log_2(\psi) +(1-\psi) \log_2(1-\psi)} $
	
	$\eta_\psi = max_x \, \eta_{x, \psi}$
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/thpsi_c8l256e4}
		\caption{$\psi = 0.05$, 2 activities per cluster, 8 clusters, 256 units per cluster, 4 erasures, each point is the mean of 10 networks with 1000 sampled messages, analytical result in continuous black}
		\label{psith}
	\end{figure}
	
	\begin{figure}[!htb]
		
		\includegraphics[scale=0.50]{Courbes/5portant_psi_c8l256e4}
		\caption{Comparison of evolutions of efficiency with increasing damages between different number of activities per cluster :  8 clusters, 256 units per cluster%, 1000 sampled messages, pools of 5 networks 
		\label{comppsi}}
	\end{figure}

	We compute values represented on figure \ref{comppsi} by scanning on the number of messages, so as to find the maximum efficiency.	
	
	After a transition, efficiencies seem to follow a linear decreasing low according to $\psi$.
	
	Absolute values of slopes diminish for increasing number of activities per cluster. We can conclude usage of multipartite cliques improve the resilience of the network.
	
	
	
	
	Figure \ref{comppsi} seems to indicate a linear relationship (after a transition for multiple activities) between $\eta_\psi -\eta_0$ and $\psi$.
	\section{Maximum theoretical efficiency}
		It was shown in \cite{Palm1980} that maximal asymptotic capacity of Willshaw networks is $\log 2$. We adapt this result to multipartite clique clustered networks.
		
		The entropy of the messages set can be approximated as $H(\mathcal{M}) \approx m c \log_2 {l \choose a}$. From $d = 1 - \left (1- \left(\frac{a}{l}\right)^2 \right)^m$, we can deduce $m \sim \frac{\log_2(1-d)}{\log_2(1-(\frac{a}{l})^2)} \sim - \frac{l^2 \log_2(1-d) \log 2}{a^2}$.
		We set $\log_2 {l \choose a} = k c$ (otherwise density approaches $0$ or $1$, and efficiency $0$). It follows that $\eta \sim \frac{m c \log_2 {l \choose a}}{l^2 {c \choose 2}} \sim \frac{k c^2 \log_2(1-d)\log 2}{a^2 {c \choose 2}}$. 
		
		$\eta \sim - 2 k  a^{-2}\log 2 \times \log_2 (1-d)$.
		
		We can bound the probability $P_{\exists e}$ for a message being "wrongly present" in the network, that is to mean adding it will not add any edge to the network albeit it is not in $\mathcal{M}$. By union-bound $P_{\exists e} \le {l \choose a}^c d^{a^2 {c \choose 2}} = 2^{c \log_ 2 {l \choose a} + a^2 {c \choose 2 }\log_2(d)} \approx 2^{c^2(k + \frac{a^2}{2} \log_2 (d))}$.
		
		Fixing $k^* = -\frac{a^2}{2} \log_2 (d)$ gives us $P_{\exists e}$ and $\eta \sim \log_2 d \log_2 (1-d) \log 2$ and therefore \[\eta_{max} = \log 2\]
	
		Multipartite cliques don't improve theoretical asymptotic efficiency (for not accepting messages that weren't previously seen by the network).

	
	\section{Conclusion}	
	
	
	
	%Marche mieux pour de petits c --> le faire sur le sparse ?
	
	
	
	\nocite{*}
	\bibliographystyle{plain} % Le style est mis entre crochets.
     \bibliography{bibli.bib}{}
\end{document}
