\documentclass[english,10pt,twocolumn]{IEEEtran}
\usepackage[utf8]{inputenc}  

\usepackage[english]{babel} 
\usepackage[T1]{fontenc}
%\usepackage{fullpage}
\usepackage[]{bbm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage{pstricks}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tikz}
\usepackage[autolanguage]{numprint}
\usepackage[affil-it]{authblk}
%\usepackage[squaren,Gray]{SIunits}
%\usepackage{tikz}
%
%\usepackage[style=numeric,backend=bibtex]{biblatex}
%\pagestyle{myheadings}


\title{Storing messages with multipartite neural cliques}
\author[]{Nissim Zerbib}
\affil{Département d'Informatique, École normale supérieure, Paris, France}

\author[]{Vincent Gripon}
\affil{Département d'Électronique, Télécom Bretagne, Brest, France}

\author{?}

\date{
	}
%

\renewcommand{\le}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\Re}{\textrm{Re}}
\renewcommand{\ge}{\geqslant}
%\newcommand{\sgn}{\textrm{le signe de }}
\newcommand{\E}{\mathcal{E}}
\newcommand{\V}{\vect{\mathcal{V}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\z}[1]{\Z/#1\Z}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathbb{U}}
\newcommand{\f}{\widehat{f}}
\newcommand{\e}[1]{e^{-2 \pi i #1}}
\newcommand{\ex}[1]{e^{2 \pi i #1}}
\renewcommand{\L}[1]{L^{#1} (\R )}
\newcommand{\leg}[2]{\left(\dfrac{#1}{#2}\right)}

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

\newcommand{\vect}[1]{\overrightarrow{#1}}
\newcommand{\disp}[1]{\displaystyle{#1}}

\setcounter{secnumdepth}{3} %pour la numérotation subsubsection
\setcounter{tocdepth}{3} %pour la numérotation dans la table de matières
\renewcommand{\theenumi}{\roman{enumi})}
%\renewcommand{\thepart}{\Alph{part}}
%\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\arabic{subsection}}
\renewcommand{\thesubsubsection}{\alph{subsubsection}}

\newcommand*\Let[2]{\State #1 $\gets$ #2}
\algrenewcommand\algorithmicrequire{\textbf{Input:}}
\algrenewcommand\algorithmicensure{\textbf{Output:}}

\newcommand{\fonction}[5]{\begin{array}{cllll}
#1 & : & #2 & \longrightarrow & #3 \\
&    & #4 & \longmapsto & #5 \end{array}}
\newcommand{\fonc}[3]{\begin{array}{lllll}
#1: & #2 & \mapsto & \displaystyle{#3}\end{array}}
\newcommand{\app}[3]{
	#1 : #2 \mapsto \displaystyle{#3}}
\newcommand{\fonct}[3]{\begin{array}{ccccc}
#1: & #2 & \longrightarrow & \displaystyle{#3}\end{array}}
\newcommand{\pent}[1]{\lfloor #1 \rfloor}

% Le point-virgule bien espacé
\newcommand{\pv}{\ensuremath{\, ; }}
% Les intervalles
   % fermé - fermé
   \newcommand{\interff}[2]{\ensuremath{\left[ #1 \pv #2 \right]}}
   % fermé - ouvert
   \newcommand{\interfo}[2]{\ensuremath{\left[ #1 \pv #2 \right[}}
   % ouvert - fermé
   \newcommand{\interof}[2]{\ensuremath{\left] #1 \pv #2 \right]}}
   % ouvert - ouvert
   \newcommand{\interoo}[2]{\ensuremath{\left] #1 \pv #2 \right[}}


\theoremstyle{definition}
\newtheorem{theoreme}{Theorem}
\renewcommand{\thetheoreme}{\arabic{section}.\arabic{theoreme}}
%\providecommand{\keywords}[1]{\textbf{Index terms---} #1}

\newcommand{\comp}[3]{\left\{
	#1\,;\, #1\in #2 \, /\, #3
	\right\}}
\newcommand{\param}[2]{\left\{
	#1\, /\, #2
	\right\}}


\begin{document}

	\maketitle

	 \begin{abstract}
	 	We extend recently introduced associative memories based on clustered cliques to multipartite cliques. We propose a variant of the classic retrieving rule. We study its performance relatively to the former one for retrieving partially erased or corrupted messages. We provide both analytical and simulation results showing improvements in both networks efficiencies and resilience to damages. We compute asymptotic capacities of these networks.
	 \end{abstract}
	 
	%\keywords{associative memory, error correcting code, cliques, multipartite cliques, neural networks}
	
	\begin{IEEEkeywords}
	associative memory, error correcting code, cliques, multipartite cliques, neural networks
	\end{IEEEkeywords}
	\section{Introduction}
	
		
		Associative memories
		
		Recently, a new type of associative memories was proposed by Gripon and Berrou \cite{GriBer20117}.
		
		GBNN can also be used to retrieve messages from erroneous versions of them. We derive a formula for error rate in case of errors on messages.
		
		Fault tolerance of those networks was extensively studied in \cite{LedGriRabGro20145}. We extend this to multipartite cliques and show significant improvements in resilience to faults.
	
	
	\section{Networks of neural cliques}
	
	An associative memory is able to store messages then retrieve them given a partial or corrupt probe.

	We first describe how messages can be stored in a recently introduced neural network based associative memory~\cite{}. Secondly, we detail how messages can be retrieved by these networks.
	
	
	\subsection{Storing messages}
	
	Consider a set $\mathcal{M}$ of messages of fixed length $c$ over a finite alphabet $\mathcal{A}$.
	
	Messages in $\mathcal{M}$ can be stored in a clustered neural network made of $c$ clusters containing $\ell$ units each. Units can take binary values : $0$ (deactivated) or $1$ (activated). A message is represented by a set of activated units in the network. %% There is a natural transport between messages and active units in the network. This transport consists in A cluster correspond to a symbol in a message and the pattern of unit activations in a cluster will determine which symbol in the alphabet the cluster represents. 
	
        One can choose the association between a message and its corresponding set of active units. In previous works~\cite{GriBer20114, GriBer20122}, each coordinate of a message $m$ is associated with a cluster. Inside a cluster, each unit is associated with a symbol in $\mathcal{A}$. As a consequence, to a message $m$ corresponds one active unit in each cluster of the network.

        In this document, we are interested in assessing the performance of such systems when several units are associated with a given symbol for each cluster. To simplify this generalisation and allow for optimized retrieval strategies, we fix this number of activated units $a$ in each cluster.
        
        Note that there are $\binom{\ell}{a}$ possible choices for $a$ activations among $\ell$ units, giving the following correspondence:
\[
\binom{\ell}{a} = | \mathcal{A} |\;.
\]
Without loss of generality, we therefore consider that $\mathcal{A}$ is composed of the list of all binary vectors containing exactly $a$ ones that we index from 1 to $\binom{\ell}{a}$: $\mathcal{A} = \{\mathcal{A}_1,\dots,\mathcal{A}_{\binom{\ell}{a}}\}$.

Previous works~\cite{GriBer20114,GriBer20122} consider the case $a=1$.
	
	%Let $\mathcal{M}$ be a set of messages of length $c$ over a finite alphabet $\mathcal{A} = \{1, 2, \hdots, {l \choose a}\}$ (where $a$ is a positive integer) and 
	 
	
	%binary constant weight codes of weight $a$.
	
	To provide convenient notations, we propose to linearise messages using the following function mapping $f$:
\[
f: \left\{\begin{array}{rcl}\mathcal{A}^c &\rightarrow& \{0,1\}^{c\cdot{} \ell}\\m&\mapsto& \mu \,\big| \forall j, \mu[(j-1)\ell+1\dots j\cdot \ell] = m_j\end{array}\right.\;,
\]
where $\mu[a\dots b]$ is the subarray of $\mu$ from coordinate $a$ to coordinate $b$.

By abusing notations, and since function $f$ is injective, we shall make no distinction between $m$ and $f(m)=\mu$.
%% A message $m$ of length $c$ can be seen as an array $x$ of length $c\cdot{} l$ where for each $j\in [c]$, the $j$-th symbol of $y$ denoted $y_j$ is represented by $x[(j-1)l+1\hdots jl]$.\footnote{This is a clustered representation. $x$ is separated in $c$ clusters of $l$ units.} To sum up the $k$-th unit in the $j$-th cluster corresponds to the index $i = (j-1)l+k$. We say the network is in state $x$ if for all $j\in [1, c]$ and $k\in [1,l]$, unit $k$ in cluster $j$ of the network is active if, and only if, $x[(j-1)l+k] = 1$. Taking into account this correspondence between messages $y$ in $\mathcal{M}$ and their clustered representation $x$, we will now also consider $x$ as messages.
	
	%Each message $x = (x_j)_{1 \le j \le c}$ is mapped to $y = (y_j)_{1 \le j \le c}$ where $y_j$ is the binary representation of the $x_i$-th $a$-combination of $l$ elements (the mapping doesn't matter as long as it is fixed).\footnote{This is a local code of constant-weight $a$.}
	
	To store a message $m$ in the network, we use the edges between units. More precisely, to store a message $\mu$ all edges between activated units are added to the network, with the exception of units in a same cluster. In other words, storing a message consists in printing a multipartite clique into the network. This process is depicted in Figure~\ref{fig:storage}. %% amounts to consider the network in state $x\,$%, that is for all $j$ and $k$ such as $1 \le k \le l$ and $1 \le j \le c$, the unit $k$ in the cluster $j$ is activated if and only if $x[(j-1)l+k] = 1$ 
	%% ; then adding all edges between activated units to the network excepting for units sharing the same cluster. 
        Notice that edges already present in the network remain unchanged throughout the storing process. %% Thus we obtain a multipartite clique between activated units representing a message.% The parameter $a$ describing the number of activated units per cluster will be called number of activities.

\begin{figure}[!h]
\centering
\begin{tikzpicture}
  \foreach \type/\fill/\xshift/\yshift in {circle/draw/-50pt/50pt,rectangle/draw/50pt/50pt,rectangle/fill/-50pt/-50pt,circle/fill/50pt/-50pt}{
    \begin{scope}[xshift=\xshift,yshift=\yshift]
    \tikzstyle{every node}=[\type,draw,\fill];
    \foreach \i in {0,20pt,40pt,60pt}{
      \foreach \j in {0,20pt,40pt,60pt}{
        \node (node\type\fill\i\j) at (\i,\j) {};
      }
    }
\end{scope}
  }
  %% \path
  %% (nodecircledraw20pt40pt) edge (nodecirclefill040pt)
  %% edge (noderectanglefill20pt20pt)
  %% edge (noderectangledraw60pt60pt)
  %% (nodecirclefill040pt) edge (noderectanglefill20pt20pt)
  %% edge (noderectangledraw60pt60pt)
  %% (noderectanglefill20pt20pt) edge (noderectangledraw60pt60pt)
  %% ;
  %% \path
  %% (nodecircledraw60pt60pt) edge (nodecirclefill40pt20pt)
  %% edge (noderectanglefill060pt)
  %% edge (noderectangledraw20pt40pt)
  %% (nodecirclefill40pt20pt) edge (noderectanglefill060pt)
  %% edge (noderectangledraw20pt40pt)
  %% (noderectanglefill060pt) edge (noderectangledraw20pt40pt)
  %% ;
  \path[thick]
  %%%%
  (nodecircledraw40pt60pt) edge (nodecirclefill20pt20pt)
  edge (noderectanglefill40pt60pt)
  edge (noderectanglefill20pt20pt)
  edge (noderectangledraw40pt20pt)
  edge (nodecirclefill60pt60pt)
  %edge (noderectanglefill40pt40pt)
  edge (noderectangledraw60pt60pt)
  %%%%%%%%%%%
  (nodecircledraw020pt) edge (nodecirclefill20pt20pt)
  edge (noderectanglefill40pt60pt)
  edge (noderectanglefill20pt20pt)
  edge (noderectangledraw40pt20pt)
  edge (nodecirclefill60pt60pt)
  %edge (noderectanglefill40pt40pt)
  edge (noderectangledraw60pt60pt)
  %%%%%%%%%%%%%%
  (nodecirclefill20pt20pt) edge (noderectanglefill40pt60pt)
  edge (noderectangledraw60pt60pt)
  edge (noderectanglefill20pt20pt)
  edge (noderectangledraw40pt20pt)
  %%%%%
  (nodecirclefill60pt60pt) edge (noderectanglefill40pt60pt)
  edge (noderectangledraw60pt60pt)
  edge (noderectanglefill20pt20pt)
  edge (noderectangledraw40pt20pt)
  %%%%%
  (noderectanglefill40pt60pt) edge (noderectangledraw60pt60pt)
  edge (noderectangledraw40pt20pt)
  %%%%%%%%%%%
  (noderectanglefill20pt20pt) edge (noderectangledraw60pt60pt)
  edge (noderectangledraw40pt20pt)
  ;
\end{tikzpicture}
\caption{Illustration of the storage process of a message $\mu$. In this setting the number of clusters is $c=4$, each containing $\ell=16$ units. The parameter $a$ is 2.}
\label{fig:storage}
\end{figure}
	
	Note that this representation is lossy, as the number of possible sets of messages is much larger than the number of possible networks.
	
	%The state of the network corresponds to a message.
	
	Formally, we can describe the network using the adjacency matrix $W$ of the underlying graph. By construction this matrix is symmetric and binary. For a message $\mu$, note that $\mu \mu^{\intercal}$ is the co-occurrence matrix associated with $\mu$. Before defining $W$, we first introduce the following matrix:
\[
	\overline{W} = \max_{\mu\in \mathcal{M}} \mu \mu^{\intercal} %% = \bigvee_{x\in \mathcal{M}} x x^{\intercal}
        \;,
\]
where $\max$ is a coefficient-to-coefficient max function. $W$ is obtained from $\overline{W}$ by removing all edges connecting units in a same cluster. We obtain a binary symmetric matrix $W$ with null blocks on the diagonal. The reason why we remove intracluster connections will be discussed in the following section.

%% $ which is the matrix where all edges between  activated units for each state $x$ in the same cluster have been added.
%% 	$W$ can now be expressed as :\vspace*{1mm}
%% \[

%% \]
	
%% 	$W_{ii'} = \bigg \{
%%   \begin{tabular}{ccc}
%%   $0$ & if $\exists j\in [1, c], \, (j-1)l+1 \le i,i' \le jl$ \\
%%   $\tilde{W}_{ii'}$ & otherwise 
%%   \end{tabular}
%%   $
%%   \vspace*{1mm}
  
  
  
  As $\max$ is associative and commutative, storing can be done online and is independent of the order in which messages are stored.
		
	\subsection{Retrieving messages}	
		
        Once a set of messages has been stored in some matrix $W$, we are interested in retrieving a stored message given a partial or corrupt probe $\tilde{\mu}$. To perform this operation, we use an iterative algorithm derived from the one proposed in~\cite{GriBer20117}.
		
%		\subsubsection{The "winner takes all" rule}
%		
%		take only maximum score
%	
%		\subsubsection{The "$w$-winners take all" rule}
		
        The algorithm relies on two steps. First it computes \emph{scores} for each unit in the network and then select which units should be activated based on their scores. For a review on different strategies for both steps in the classical case where $a=1$, consider~\cite{Ala}. To adapt this algorithm in the case $a\ge 2$, we only modify the second step for choosing which units to be activated.

        There are several strategies for computing scores of the units, but our simulations suggest that their impact on performance is not significative. We therefore use what is called the SUM-OF-SUM rule in which scores are computed as follows:

\[
s = \gamma \tilde{\mu} + W \tilde{\mu}\;.
\]

In other words, the score of a unit is the number of activated units it is connected to, plus a memory effect with parameter $\gamma$. This memory effect can have significative impact on performance and its optimal value depends on the problem considered. This is the main reason why intracluster edges are forbidden.

        Once scores are computed, we are interested in choosing which units to be activated. In order to account for the fact several units should be activated in each cluster, we propose to activate the $\alpha$ units achieving the maximum score in each cluster. Note that in case of equality of scores, we select all tied units resulting in possibly more than $\alpha$ units activated in each cluster. Intuitively, the best possible choice for $\alpha$ is $a$, but it implies to have some prior knowledge about the value of $a$. If one does not know the value of $a$, choosing $\alpha=1$ should lead to reasonable performance.

		%% In order to account for the changes to the model, we make a small modification of the classic retrieving rule. Indeed, we keep activated all units that achieve a score equal or greater than the $w$-th score. (where $w$ is a positive integer parameter). $w = 1$ amounts to taking the maximum and so is exactly the same as the classic retrieving rule.
		
		%$a$-rank
		
		%Although it is a less biologically plausible rule, it has the same algorithmic complexity, since selecting the $w$-th greater element in an array of $n$ elements is $O(n)$ in complexity.
		
		%% Intuitively the choice of $w = a$ is optimal, as each cluster should eventually contain exactly $a$ active units to at least represent a symbol in $\mathcal{A}$. %(it it is the most natural one with $w = 1$ as well).
		%% %Algorithm \ref{asumsum} is a description of the algorithm\footnote{No optimizations are described here (early stopping etc.)} :
		
		
	Formally, we introduce a function $\mbox{select}$ such as for an array $\tilde{\mu}$, $\mbox{select}(\alpha, \tilde{\mu})$ is equal to the $\alpha$-th greatest element of $\mu$ (counting repetitions): as an example $\mbox{select}(3, [4,2,1,2,0,2]) = 2$. Then the selection of activated units is performed as follows:
		
		\begin{align*}
		&\tilde{\mu}'[(j-1)\ell+k] =\\ \bigg \{
  			&\begin{tabular}{cl}
  			$1$ & if $s[(j-1)\ell +k] \ge  \mbox{select}(\alpha, s[(j-1)\ell+1 \hdots j\ell])$ \\
  			$0$ & otherwise 
  		         \end{tabular}\;.
  		\end{align*}
                Thus we use an Heavyside function with a parameter depending on each cluster. $\tilde{\mu}'$ is the output of the algorithm after one iteration. The process can be repeated providing $\tilde{\mu}'$ as the new input to the network. Several stopping criterion can be used~\cite{Ala}. In the remaining of this work, we will consider the number of iterations to be fixed. Simulations show that increasing the number of iterations can result in significatively better performance.

The network is considered to succeed if the final output of the algorithm matches the original message of which $\tilde{\mu}$ is a partial or corrupted version of.
		
		%% We can for example fix a maximum number of steps to the algorithm or stop at one point if $\tilde{x}_{t+1} = \tilde{x}_t$. Increasing the number of iterations can of course improve the retrieval rate.
		
		%% We consider an altered version $\tilde{x}$ of a message $x$ which we are trying to retrieve. We set $\tilde{x}^0 = \tilde{x}$. The network is in state $\tilde{x}$ at the beginning of the retrieving. The state of the network at the beginning of the step $t$ will be denoted by $\tilde{x}_t$. The message will be successfully retrieved if at the end of the algorithm, the network is in state $x$.
		
		%% At each step $t$, we compute the scores, that is to mean the number of active units each unit is connected to, plus a memory effect $\gamma $ accounting for units begin already active. The array of scores at step $t$ is :  $s_{t} = \gamma \tilde{x_{t}} + W \tilde{x_{t}}$
		
		
		
%		\begin{algorithm}[!htb]
%		\caption{$w$-sum of sum \label{asumsum}}
%		\begin{algorithmic}[1]
%		\Require{$v$ a binary array of length $n = c\cdot l$, $W$ the binary weight matrix representing the network, $S$ the number of iterations, $\gamma$ memory effect}
%		\Procedure{Sum of sum}{$v, W, \gamma, S, w$}
%		%\Statex
%		\For{$s$ from $1$ to $S$}
%		\State $s \gets \gamma v + W v$
%		\For{$i$ from $1$ to $c$}
%			\Let{treshold}{Select(s[$i$], $w$)} \Comment{Select(t, w) returns the element that would be in rank $w$ if the array t was sorted}
%			\For{$j$ from $1$ to $l$}
%			\Let{$v[i][j]$}{$s[i][j]$ $\ge$ treshold}
%			\EndFor
%		\EndFor
%		\EndFor
%		%\State\Return v
%		\EndProcedure
%		\end{algorithmic}
%		\end{algorithm}
	
	We now describe some static properties of the networks as the density of edges.
	
	\section{Static properties of multipartite clique networks}	
	
	We study the properties of the network after storing messages in it. We are interested in how much edges have been added to the network and if the process introduced confusion between messages.	
	
	Messages stored in the network will be considered independent and identically distributed according to the uniform distribution. This randomization allows us to compute the theoretical density of the network. %under the approximation that edges are independent. 
	Density is a crucial parameter of the network that can be used to predict retrieval rates for different problems. Moreover we compute asymptotic efficiency thanks to the formula for density.
	
	\subsection{Density}
		
	The density $d$ of the network is the proportion of edges present in the network : $d = \frac{\# \{(i, i') \} \, | W_{ii'} = 1 \}}{c(c-1)\ell^2}$.
	
	\subsubsection{Theoretical density}
	
	We first compute the theoretical density of the network, that is to mean the probability for a random edge $(i, i')$ to be present in the network (i.e. $W_{ii'} = 1$). 
	
	%number of neurons per cluster $l$
	
	%number of clusters $c$
	
	%number of erased clusters $c_e$
	
	%number of messages : $m$
	
	%number of activities per cluster : $a$
	
	%density : $d$
	
	%edge : $(i, j)$
	
	%We generalise results on erasures obtained in \cite{GriBer20117} to multipartie cliques.
	

	
	
	As the probability for $i$ (and $i'$) to be active in its cluster for one message $\mu$ is $a/\ell$ providing messages are uniformly distributed, it follows that the probability for the edge $(i,i')$ of not being added in the network for $\mu$ is $1 - \left(a/l\right)^2$. Since messages are independent, the density $d$, which is the probability for an edge to be in the network, can be expressed as :
	
	\begin{align}
		\label{formula_density}
		d = 1 - \left( 1 - \left(\frac{a}{\ell}\right)^2 \right)^M 
	\end{align}		
	, recalling that $M$ is the number of messages in $\mathcal{M}$.	
	
	\subsubsection{Simulations}
	
	Theoretical and empirical densities match very closely, see Figure \ref{densiteth}. %gap of theoretical density with density in simulations : under $1$ percent, 
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/remplacement_densite_c8l256a4}
		\caption{Theoretical and empirical densities for a network of 8 clusters, 256 units per cluster and 4 activated units per cluster}
			\label{densiteth}
	\end{figure}	
	
	\subsection{Accepting random messages}
		
	\section{Maximum theoretical efficiency}
		It was shown in \cite{Palm1980} that maximal asymptotic capacity of Willshaw networks is $\log 2$. We adapt this result to multipartite clique clustered networks.
		
		The entropy of the messages set can be approximated as $H(\mathcal{M}) \approx M c \log_2 {\ell \choose a}$. From $d = 1 - \left (1- \left(a/\ell\right)^2 \right)^M$, we can deduce $M \sim \frac{\log_2(1-d)}{\log_2(1-(\frac{a}{\ell})^2)} \sim - \frac{\ell^2 \log_2(1-d) \log 2}{a^2}$.
		We set $\log_2 {\ell \choose a} = k c$ (otherwise density approaches $0$ or $1$, and efficiency $0$). It follows that $\eta \sim \frac{M c \log_2 {\ell \choose a}}{\ell^2 {c \choose 2}} \sim \frac{k c^2 \log_2(1-d)\log 2}{a^2 {c \choose 2}}$. 
		
		$\eta \sim - 2 k  a^{-2}\log 2 \times \log_2 (1-d)$.
		
		We can bound the probability $P_{\exists e}$ for a message being "wrongly present" in the network, that is to mean adding it will not add any edge to the network albeit it is not in $\mathcal{M}$. By union-bound $P_{\exists e} \le {\ell \choose a}^c d^{a^2 {c \choose 2}} = 2^{c \log_ 2 {l \choose a} + a^2 {c \choose 2 }\log_2(d)} \approx 2^{c^2(k + \frac{a^2}{2} \log_2 (d))}$.
		
		Fixing $k^* = -\frac{a^2}{2} \log_2 (d)$ gives us $P_{\exists e}$ and $\eta \sim \log_2 d \log_2 (1-d) \log 2$ and therefore \[\eta_{max} = \log 2\]
	
		Multipartite cliques don't improve theoretical asymptotic efficiency (for not accepting messages that weren't previously seen by the network).


	We now describe the distinct problems of messages retrieval that we study the performance of.	
	
	\section{Problems statements}		
	
	\subsection{Erasures}
	
	\subsection{Random replacements}
	
	\subsection{Resilience}
	
	
	
	We now study the performance of the new retrieving rule on those distinct problems of retrieval : first partially erased messages, then erroneous messages.%, finally partially erased messages when the network has been damaged.	
	
	\section{Retrieval performance}	
	
	Thanks to the theoretical formula for density, we compute analytical error rates after one iteration of the retrieving rule. We compare those results to empirical error rates obtained through simulations\footnote{Simulations are done by taking means on pool of five networks and messages sets, and each point is drawn for a different network and messages set.}. We also compare efficiencies of distinct networks for distinct values of the parameter $a$ (weight of the constant weight code) in the sense of information theory and show improvements over the case where $a = 1$.
	
	
	
		
	
	\subsection{Retrieving partially erased messages}		
		
	One can see a partial version of a message $x$ as the result of erasures of symbols of $x$. Some symbols of $x$ are replaced by the blank character $\intercal$.	Erasures of a symbol in a message corresponds to an "erased" cluster in the network. An erased cluster is a cluster of units which are all inactive, that is to mean set to $0$.
	
	\subsubsection{Analytical result}
	
	We then express the theoretical error rate after one iteration of the retrieving rule in function of the density. All this computations (on error rates) are done assuming existences of edges between random units are independent, which is false but is a convenient approximation that gives results matching very closely results from simulations.
	
	Suppose we are trying to retrieve a message where $c_e$ clusters have been erased. %(i.e. all units in those clusters are set to $0$).
	Thanks to the memory effect $\gamma$, activated units in a non-erased cluster stay activated and are the only ones being so in their cluster (as the others can't achieve a higher score thanks to not being already activated). For units in erased clusters, the maximum attainable score is $a(c - c_e)$. Units in erased clusters corresponding to the original message achieve this score. Since scores follow a binomial law, the probability for a random unit to achieve the maximum score in such a cluster, that is to mean $a(c-c_e)$ is $d^{a(c-c_e)}$.
	
	%probability for a vertex $v$	not to achieve the maximum : $1 - d^	{a(c-c_e)}$
	
	Since unit activations are independent (as messages are themselves independent), the probability for none of the vertices of one erased cluster, excepting the correct ones, to achieve the maximum is $\left(1 - d^	{a(c-c_e)}\right)^{\ell-a}$.
	
	Scores in clusters being also independent, the probability for none of the vertices in any erased cluster, excepting the correct ones, to achieve the maximum in their respective clusters is $\left(1 - d^	{a(c-c_e)}\right)^{c_e(\ell-a)}$.
	
	Whence error rate in retrieving messages is : 
	\begin{align}	
	P_{err} = 1 -	\left(1 - d^	{a(c-c_e)}\right)^{c_e(\ell-a)} 
	\end{align}	 
	
	In order to compare distinct networks using distinct alphabets (due to variations of the weight parameter $a$), we define the efficiency of the network relatively to the hardness of the problem (how much it is efficient compared to a perfect associative memory). A perfect associative memory would use exactly the same number of bits than the original messages and retrieve messages by maximum likelihood (ambiguities causing errors). Efficiency is the amount of information retrieved divided by the hardness of the problem.
	
	The network is a binary symmetric matrix with null diagonal and can therefore be stored in memory as $\frac{c(c-1) \ell^2}{2}$ bits. Since messages are not stored in an ordered fashion, the entropy of the set of messages is $\log_2(\frac{({\ell \choose a}^c)^M}{M!}) \mathop{=}_{M \rightarrow +\infty} M(c \log_2{\ell \choose a } - \log_2(M) + \frac{1}{\log 2} + o(1)) $ (thanks to  Stirling's formula). The ratio of information effectively stored in the network is $P_{retrieve} \times  \frac{2M \left(c \log_2{\ell \choose a } - \log_2(M) + \frac{1}{\log 2} \right)}{c(c-1)\ell^2}$.
	
	Ambiguities would occur in a perfect associative memory if at least two messages were identical over non-erased letters. The probability it doesn't happen is $ (1-{\ell \choose a}^{c - c_e})^{M-1}$.
	
	
	$\eta_M = P_{retrieve}  \frac{2 M\left(c \log_2{\ell \choose a } - \log_2(M) + \frac{1}{\log 2} \right)}{c(c-1)\ell^2} \times (1-(\frac{1}{{\ell \choose a}})^{c - c_e})^{-(M-1)}$
	%\frac{c}{(c-c_e) \log_2{l \choose a }}
	
	The maximum ratio of information that can be stored in the network is therefore $\eta = \max_x \eta_x $, which we will define as the efficiency of the network.
	
	\subsubsection{Simulations}
		
		
		Simulations agree with the analytical results for error rate, albeit we can observe a small shift, see Figure \ref{erasuresth}. The theoretical error rate is a bit optimistic which is due to the connections not being independent.
		
		 For one iteration, as long as the parameter $w$ is between $1$ and $a$, there is no effect on retrieving. However for $4$ iterations, the choice of $w = a$ allows a shift of about $5000$ messages stored on top of those stored for $w = 1$. The variant rule can thus be considered a significant improvement over the classic one for multipartite clique networks (recall that $w = 1$ amounts to the classic retrieving rule).
		
		
	

	%For one iteration (for erasures, not for errors), "winner takes all" is the same as "$a$-winners take all".
	
	%For multiple iterations $a$-winners take all is a significant improvement.
	%The variant of the rule significantly improves the retrieval rate over multiple iterations. (For one iteration, is is the same as taking only maximum scores.)
	
	%See Figure \ref{erasuresth}	
	
		
	
	
	
%	\begin{figure}[!htb]
%		\includegraphics[width=8.5cm]{Courbes/remplacement_figure2g1} %{comparaison_regles_pool5_a2c4l512e2}
%		\caption{Evolution of error rate for increasing number of stored messages, 2 activities per cluster, 4 clusters, 512 units per cluster, two erasures, each point is the mean of 5 networks with 1000 sampled messages, analytical result (for one iteration) in continuous black}
%			\label{erasuresth}
%		\end{figure}
		
		\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/fig3c4l512e2a2} %{comparaison_regles_pool5_a2c4l512e2}
		\caption{Evolution of error rate for increasing number of stored messages, 2 activities per cluster, 4 clusters, 512 units per cluster, two erasures, each point is the mean of 10 networks with 1000 sampled messages, analytical result (for one iteration) in continuous black}
			\label{erasuresth}
		\end{figure}		
		
		Thanks to the efficiency defined in the previous section, we can easily compare distinct choices of parameters $a$ (we now take $w = a$ since it is the most efficient choice in practice) over distinct number of erased clusters. Figure \ref{comperth} shows improvements in efficiencies over parameter $a =1$. The more difficult the problem is, the more redundancies introduced by multipartite cliques are useful.
		
		\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/5portant_erasures_c8l256}
		\caption{Comparison between different number of activities per cluster :  8 clusters, 256 units per cluster, 1000 sampled messages, pools of 5}
		\label{comperth}
	\end{figure}
%	
%	\newpage
%	$\,$
%	\newpage
%	$\,$
%	\newpage
	
	Messages can suffer many types of alteration. Multipartite clique neural networks can also correct errors.	
	
	\subsection{Retrieving corrupted messages}
	
	\subsubsection{Analytical result}
%	Formula for errors instead of erasures : 
%	
%	gc : good (à remplacer par right/wrong ?) unit in correct cluster
%	
%	ge : good unit in erroneous cluster
%	
%	abe : bad activated unit in erroneous cluster
%	
%	be : others bad (à remplacer par wrong ou incorrect etc.) units in erroneous cluster
%	
%	bc : bad unit in correct cluster
	
	We provide a formula for error rate in case of messages with corrupted clusters.

	Messages can be corrupted, our models for errors is : given $c_e$ the number of erroneous clusters, we draw at random $c_e$ clusters and in each of these draw at random, uniformly, a new letter in the alphabet that will replace the old one (it could be the same).
	
	Knowing the density of the network, which follows formula (\ref{formula_density}), we can compute the laws of probabilities for scores achieved by different type of units in different clusters. We can divide units in 5 groups with 3 characteristics. Subscript $g$ will indicate a unit being part of the message (it should be activated), subscript $b$ the contrary (the unit should not be activated). Subscript $a$ means the unit is already activated. Subscript $c$ corresponds to a correct cluster, $e$ to an erroneous cluster.
	Correct units in uncorrupted clusters achieve at least a score of $a(c-c_e - 1) + \gamma$. 
	Being connected to a given activated unit in a corrupted cluster occurs with probability $d$, since errors are identically distributed. Moreover corrects units see their scores shift, as do all activated units respectively thanks to being part of the original message and the memory effect $\gamma$. We can express those probability laws as : 
	For example an \textbf{a}ctivated \textbf{g}ood unit in a \textbf{c}orrect cluster starts with a base score of $ a(c - c_e - 1) + \gamma$. The probability it has to be connected to a random activated unit in an erroneous cluster is $d$. Thereby its law is :
	
	%$P(n_{agc} = (c - c_e - 1) + \gamma + x) = {c_e \choose x} d^x (1-d)^{ce-x}$
	$P(n_{agc} = a(c - c_e - 1) + \gamma + x) = {a c_e \choose x} d^x (1-d)^{a ce-x}$
	
	Correct units in corrupted clusters achieve at least a score of $a(c - c_e)$. 
	$P(n_{ge} = a(c - c_e) + x) = {a (c_e - 1) \choose x} d^x (1-d)^{a (c_e - 1)-x}$
	
	%$P(n_{ge} = (c - c_e) + x) = {c_e - 1 \choose x} d^x (1-d)^{c_e-1-x}$
	
	Activated units benefits of the memory effect $\gamma$ :	\\
	$P(n_{abe} = \gamma + x) = {a(c - 1) \choose x} d^x (1-d)^{a(c-1)-x}$
	%$P(n_{abe} = \gamma + x) = {c - 1 \choose x} d^x (1-d)^{c-1-x}$
	
	
	%$P(n_{be} = x) = P(n_{bc} = x) = {c - 1 \choose x} d^x (1-d)^{c- 1 -x}$
	Finally $P(n_{be} = x) = P(n_{bc} = x) = {a(c - 1) \choose x} d^x (1-d)^{a(c- 1) -x}$	
	
	A message is retrieved after one iteration, if and only if correct units achieve strictly the best scores.	
	
%	\begin{align}
%	P_{retrieve} = \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{agc} = n) \left [ \sum_{x=0}^{n-1} P(n_{bc} = x) \right]^{l-1} \right ]^{c - c_e} \\ \times \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{ge} = n) \left [ \sum_{x=0}^{n-1} P(n_{bc} = x) \right]^{l-2} \left [ \sum_{x=0}^{n-1} P(n_{abc} = x) \right] \right ]^{c_e}
%	\end{align}
	
%	\begin{align*}
%	P_{retrieve} = \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{agc} = n)  P(n_{bc} < n)^{l-1} \right ]^{c - c_e} \\ \times \left [ \sum_{n = 1}^{c-1+\gamma} P(n_{ge} = n) P(n_{bc} < n)^{l-2} P(n_{abc} < n) \right ]^{c_e}
%	\end{align*}

	
	%à changer : prendre en compte les $a$ unites activés : problème des recouvrements !! on est trop optimiste si grand nombre d'activités (ou pessimiste ?)
	
	
	
	The probability for all good units of a correct cluster (resp. of an erroneous cluster) to achieve a score equal or greater than $x$, with at least one having a score of $x$ is $P_{ac} (x) = \sum_{k = 1}^{a} { a \choose k }   P(n_{agc} = x)^k P(n_{agc} > x)^{a-k}$ (resp. $P_e(x) = \sum_{k = 1}^a { a \choose k } P(n_{ge} = x)^k P(n_{ge} > x)^{a-k}$)

	
	
%	\begin{align*}
%	&P_{retrieve} = \left [ \sum_{n = 1}^{a(c-1) + \gamma} \left [ \sum_{k = 1}^{a} { a \choose k }   P(n_{agc} = n)^k P(n_{agc} > n)^{a-k} \right ] P(n_{bc} < n)^{l-a} \right ]^{c - c_e}\\ 
%	 &\times \left [ \sum_{n = 1}^{a(c-1) +\gamma} \left[ \sum_{k = 1}^a { a \choose k } P(n_{ge} = n)^k P(n_{ge} > n)^{a-k} \right ] P(n_{be} < n)^{l-2a} P(n_{abe} < n)^a \right ]^{c_e} \numberthis \label{from_err_th}
%	\end{align*}
		\begin{align*}
	&P_{retrieve} = \left [ \sum_{x = 1}^{a(c-1) + \gamma} P_{ac}(x) P(n_{bc} < x)^{\ell-a} \right ]^{c - c_e}\\ 
	 &\times \left [ \sum_{x = 1}^{a(c-1) +\gamma} P_e(x) P(n_{be} < x)^{\ell-2a} P(n_{abe} < x)^a \right ]^{c_e} \numberthis \label{from_err_th}
	\end{align*}
	
	with the approximation that there is few activated units that should be kept active in an erroneous cluster.

%	To be really precise : (ne pas oublier de tenir compte changement des lois, formule inexacte pour l'instant)
%	\begin{align*}
%	&P_{retrieve} = \left [ \sum_{n = 1}^{a(c-1) + \gamma} \left [ \sum_{k = 1}^{a} { a \choose k }   P(n_{gc} = n)^k P(n_{gc} > n)^{a-k} \right ] P(n_{bc} < n)^{l-a} \right ]^{c - c_e}\\ 
%	 &\times \left [ \sum_{y = 0}^a \left( \frac{{l-y \choose a} - \sum_{z = y+1}^{a} {l-z \choose a }}{{l \choose a}}\right)\sum_{n = 1}^{a(c-1) +\gamma} \left[ \sum_{k = 1}^a { a \choose k } P(n_{ge} = n)^k P(n_{ge} > n)^{a-k} \right ] P(n_{bc} < n)^{l-a - y} P(n_{abc} < n)^{a - y} \right ]^{c_e}
%	\end{align*}	
	
	$P_{err} = 1 - P_{retrieve}$	

	
	
	\subsubsection{Simulations}
	See Figure \ref{corruptth} compares the two choices $w = 1$ and $w = a$.
	"$a$-winners take all" better than "winner takes all" with errors instead of erasures, even for one iteration.
	
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/fig4c4l512a2e1corr}%remplacement_figure3g1}
		\caption{2 activities per cluster, 4 clusters, 512 units per cluster, one erroneous cluster, each point is the mean of 10 networks with 1000 sampled messages, analytical result in continuous black}
		\label{corruptth}
	\end{figure}
		
		$\eta_m = P_{retrieve}  \frac{2 M\left(c \log_2{\ell \choose a } - \log_2(M) + \frac{1}{\log 2} \right)}{c(c-1)\ell^2} \times \left [\sum_{k = c_e+1}^{c} \left (1-\frac{1}{{\ell \choose a}} \right )^k \left (\frac{1}{{\ell \choose a}} \right )^{c-k} \right ] ^{-(M-1)}$
		
		

	
	\section{Resilience}
	\subsection{Analytical result}
	%Spéculations en Français 
	%Peut-être un intérêt pour cluter based associative memories build from unreliable storage : si une arête porte moins d'info, on peut peut-être en supprimer plus (mais rajout ?!)
	
	As information on a message is shared across edges with multiple activated units per cluster, such networks are more resilient to damages (for one message information is carried by $a^2 {c \choose 2}$ edges. $v_c$ will indicate a correct unit that should be activated, $v$ the other type of units.
	
	Our deviation model is the same as \cite{LedGriRabGro20145}\footnote{Compared to this article, we do not draw at random a letter in case of ambiguity, which hurts performance a bit.}, that is random binary noise on edges. We generalise results of \cite{LedGriRabGro20145} to multipartite cliques.
	
	%gamma = infini meilleur sur une itération
	We assume $\gamma = \infty$ for one iteration as it prevents activated neurons to be deactivated.
	With probability $1 - \psi$, one correct vertex is still connected to a given activated unit in an intact cluster. Thus $P(n_{v_c} = x ) = {a (c_k-1) \choose x} (1-\psi)^{x} \psi ^ { a (c - c_e -1) - x }$	
	
	%$P(n_{v_c} = x) = {a c_k \choose x} (1-\psi)^{x} \psi ^ { a (c - c_e) - x }$
	
	The probability for an edge to be present in the network after damages is : $P_+ = \psi (1 - d) + (1 - \psi) d$. So for other units $P(n_v = x) = {a (c - c_e) \choose x} P_+^x (1-P_+)^{a (c - c_e) -x }$.
	
	% À corriger avec sommes multiparties
%	$P(\mbox{no other vertex activated in this cluster})= \sum_{n_0 = 1}^{a (c - c_e)} P(n_{v_c} \ge n_0)^a \left [ \sum_{x = 0}^{n_0 - 1} P(n_v = x) \right]^{l-a}$
%	
	% Approx : ?! $P(\mbox{no other vertex activated in any cluster})=  \left ( \sum_{n_0 = 1}^{a ( c - c_e)} P(n_{v_c} \ge n_0)^a \left [ \sum_{x = 0}^{n_0 - 1} P(n_v = x) \right]^{l-a } \right)^{c_e}$
	
	%Sans tenir compte du choix au hasard si plus sont activés. Pour une itération $\gamma = \infty$. Pour plusieurs $\gamma = 1$.
	
	The probability for all correct units to achieve a score equal or greater than $x$, with at least one achieving a score of $x$ is $P_c (x) =\sum_{k = 1}^{a} { a \choose k }   P(n_{v_c} = x)^k P(n_{v_c} > x)^{a-k} $

	%If $\gamma$ is big enough,
	It follows that $P(\mbox{no other vertex activated in one cluster})= \sum_{x = 1}^{a (c - c_e)} P_c(x) P(n_v < x)^{\ell-a}$.%[ \sum_{x = 0}^{n - 1} P(n_v = x) \right]^{l-a }$
	
	%P(\mbox{no other vertex activated in any cluster})
%	\begin{align*}
%	P_{retrieve}=  \Bigg ( \sum_{n = 1}^{a ( c- c_e)} \left [ \sum_{k = 1}^{a} { a \choose k }   P(n_{v_c} = n)^k P(n_{v_c} > n)^{a-k} \right ]\\
%	\left [ \sum_{x = 0}^{n - 1} P(n_v = x) \right]^{l-a } \Bigg)^{c_e}	
%	\end{align*}

	\begin{align}
	P_{retrieve}=  \left( \sum_{x = 1}^{a ( c- c_e)} P_c(x) P(n_v < x)^{\ell-a} \right )^{c_e}
	\label{psi_formula_th}
	\end{align}

	\subsection{Simulations}
	
	With random noise on edges, hardness of the problem must take into account the capacity of the binary symmetric channel (the closer $\psi$ is of $0.5$, the harder the problem is). Our new is efficiency is written as :  	
	$\eta_{m, \psi} = P_{retrieve}  \frac{2 M \left(c \log_2{\ell \choose a } - \log_2(M) + \frac{1}{\log 2} \right)}{c(c-1)l^2} \times (1-{\ell \choose a}^{c - c_e})^{-(M-1)} \times \frac{1}{1 + \psi \log_2(\psi) +(1-\psi) \log_2(1-\psi)} $
	
	$\eta_\psi = max_x \, \eta_{x, \psi}$
	\begin{figure}[!htb]
		\includegraphics[width=8.5cm]{Courbes/figpsic8l256e4psi2percent}%thpsi_c8l256e4}
		\caption{$\psi = 0.02$, 2 activities per cluster, 8 clusters, 256 units per cluster, 4 erasures, each point is the mean of 10 networks with 1000 sampled messages, analytical result in continuous black}
		\label{psith}
	\end{figure}
	
	\begin{figure}[!htb]
		
		\includegraphics[scale=0.50]{Courbes/5portant_psi_c8l256e4}
		\caption{Comparison of evolutions of efficiency with increasing damages between different number of activities per cluster :  8 clusters, 256 units per cluster%, 1000 sampled messages, pools of 5 networks 
		\label{comppsi}}
	\end{figure}

	We compute values represented on figure \ref{comppsi} by scanning on the number of messages, so as to find the maximum efficiency.	
	
	After a transition, efficiencies seem to follow a linear decreasing low according to $\psi$.
	
	Absolute values of slopes diminish for increasing number of activities per cluster. We can conclude usage of multipartite cliques improve the resilience of the network.
	
	
	
	
	Figure \ref{comppsi} seems to indicate a linear relationship (after a transition for multiple activities) between $\eta_\psi -\eta_0$ and $\psi$.
	
	
	\section{Conclusion}	
	
	
	
	%Marche mieux pour de petits c --> le faire sur le sparse ?
	
	
	
	\nocite{*}
	\bibliographystyle{plain} % Le style est mis entre crochets.
     \bibliography{bibli.bib}{}
\end{document}
