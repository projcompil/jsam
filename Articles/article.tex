\documentclass[english,11pt,twocolumn]{article}
\usepackage[utf8]{inputenc}  

\usepackage[english]{babel} 
\usepackage[T1]{fontenc}
%\usepackage{fullpage}
\usepackage[]{bbm}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
%\usepackage{pstricks}
\usepackage{float}
\usepackage[autolanguage]{numprint}
%\usepackage[squaren,Gray]{SIunits}
%\usepackage{tikz}
%
%\usepackage[style=numeric,backend=bibtex]{biblatex}
\pagestyle{myheadings}


\title{Storing messages with multipartite neural cliques}
\author{???}
\date{
	}
%

\renewcommand{\le}{\leqslant}
\renewcommand{\epsilon}{\varepsilon}
\renewcommand{\Re}{\textrm{Re}}
\renewcommand{\ge}{\geqslant}
%\newcommand{\sgn}{\textrm{le signe de }}
\newcommand{\E}{\mathcal{E}}
\newcommand{\V}{\vect{\mathcal{V}}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\z}[1]{\Z/#1\Z}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathbb{U}}
\newcommand{\f}{\widehat{f}}
\newcommand{\e}[1]{e^{-2 \pi i #1}}
\newcommand{\ex}[1]{e^{2 \pi i #1}}
\renewcommand{\L}[1]{L^{#1} (\R )}
\newcommand{\leg}[2]{\left(\dfrac{#1}{#2}\right)}

\newcommand{\vect}[1]{\overrightarrow{#1}}
\newcommand{\disp}[1]{\displaystyle{#1}}

\setcounter{secnumdepth}{3} %pour la numérotation subsubsection
\setcounter{tocdepth}{3} %pour la numérotation dans la table de matières
\renewcommand{\theenumi}{\roman{enumi})}
\renewcommand{\thepart}{\Alph{part}}
%\renewcommand{\thechapter}{\arabic{chapter}}
\renewcommand{\thesection}{\Roman{section}}
\renewcommand{\thesubsection}{\arabic{subsection}}
\renewcommand{\thesubsubsection}{\alph{subsubsection}}

\newcommand{\fonction}[5]{\begin{array}{cllll}
#1 & : & #2 & \longrightarrow & #3 \\
&    & #4 & \longmapsto & #5 \end{array}}
\newcommand{\fonc}[3]{\begin{array}{lllll}
#1: & #2 & \mapsto & \displaystyle{#3}\end{array}}
\newcommand{\app}[3]{
	#1 : #2 \mapsto \displaystyle{#3}}
\newcommand{\fonct}[3]{\begin{array}{ccccc}
#1: & #2 & \longrightarrow & \displaystyle{#3}\end{array}}
\newcommand{\pent}[1]{\lfloor #1 \rfloor}

% Le point-virgule bien espacé
\newcommand{\pv}{\ensuremath{\, ; }}
% Les intervalles
   % fermé - fermé
   \newcommand{\interff}[2]{\ensuremath{\left[ #1 \pv #2 \right]}}
   % fermé - ouvert
   \newcommand{\interfo}[2]{\ensuremath{\left[ #1 \pv #2 \right[}}
   % ouvert - fermé
   \newcommand{\interof}[2]{\ensuremath{\left] #1 \pv #2 \right]}}
   % ouvert - ouvert
   \newcommand{\interoo}[2]{\ensuremath{\left] #1 \pv #2 \right[}}


\theoremstyle{definition}
\newtheorem{theoreme}{Theorem}
\renewcommand{\thetheoreme}{\arabic{section}.\arabic{theoreme}}
\providecommand{\keywords}[1]{\textbf{Index terms---} #1}

\newcommand{\comp}[3]{\left\{
	#1\,;\, #1\in #2 \, /\, #3
	\right\}}
\newcommand{\param}[2]{\left\{
	#1\, /\, #2
	\right\}}


\begin{document}

	\maketitle

	 \begin{abstract}
	 ???
	 \end{abstract}
	 
	\keywords{associative memory, error correcting code}
	
	\section{Introduction}
	
	\section{Networks of neural cliques}
		
	\subsection{Learning messages}
	
		\subsection{Retrieving messages}	
	
		\subsubsection{The "winner takes all" rule}	
	
		\subsubsection{The "$a$ winners take all" rule}
		
		less biologically plausible rule
		
		same algorithmic complexity
	

	\section{Retrieval performance}	
	

		\subsection{Analytical results}		
		
		
		
	
	number of neurons per cluster $l$
	
	number of clusters $c$
	
	number of erased clusters $c_e$
	
	number of messages : $m$
	
	number of activities per cluster : $a$
	
	density : $d$

	edge : $(i, j)$
	
	probability for $i$ and $j$ to be active in their respective clusters for one message : $\frac{a}{l}$
	
	probability of the edge	not being added in the network for one message : $1 - \left(\frac{a}{l}\right)^2$
	
			
	\[ d = 1 - \left( 1 - \left(\frac{a}{l}\right)^2 \right)^m \]
	
	gap with density in simulations : under $1$ percent
	
	probability for a vertex $v$ to achieve the maximum score, that is to mean $a(c-c_e)$ : $d^	{a(c-c_e)}$
	
	probability for a vertex $v$	not to achieve the maximum : $1 - d^	{a(c-c_e)}$
	
	probability for none of the vertices of one erased cluster, excepting the correct ones, to achieve the maximum : $\left(1 - d^	{a(c-c_e)}\right)^{l-a}$
	
	probability for none of the vertices in any erased cluster, excepting the correct ones, to achieve the maximum in their respective cluster : $\left(1 - d^	{a(c-c_e)}\right)^{c_e(l-a)}$
	
	Whence error rate is : \[P_{err} = 1 -	\left(1 - d^	{a(c-c_e)}\right)^{c_e(l-a)} \]
	
	
	
	
	
	\subsection{Simulations}
		Improves the retrieval rate for multiple iterations and/or corrupted messages.
		
		The simulations agree with the analytical results for density and error rate.
	

	For one iteration (for erasures, not for errors), winner takes all is the same as $a$-winners take all.
	
	For multiple iterations $a$-winners take all is a significant improvement.
	
		"$a$-winners take all" better than "winner takes all" with errors instead of erasures, even for one iteration.
	
	For errors : only better than only 1 activity per cluster if multiple iterations of the $a$-winners take all rule.
	For one iteration, it is less efficient.
	\begin{figure}
		\includegraphics[width=8.5cm]{densiteexemple.pdf}
		\caption{Theoretical and empirical densities}
	\end{figure}
	
	\begin{figure}
		\includegraphics[width=8.5cm]{comparaison_regles_pool5_a2c4l512e2}
		\caption{2 activities per cluster, 4 clusters, 512 neurons per cluster, two erasures, each point is the mean of 5 networks with 1000 sampled messagges, analytical result in continuous black}
		\end{figure}	
		
	\begin{figure}
		\includegraphics[width=8.5cm]{comp_erreurs_a2l512c4er1}
		\caption{2 activities per cluster, 4 clusters, 512 neurons per cluster, two erasures, each point is the mean of 5 networks with 1000 sampled messagges, analytical result in continuous black}
	\end{figure}
		
	\newpage
	Spéculations en Français :
	
	Peut-être un intérêt pour cluter based associative memories build from unreliable storage : si une arête porte moins d'info, on peut peut-être en supprimer plus (mais rajout ?!)
	

	Marche mieux pour des bruits importants
	
	$P(n_{v_c} = n_0) = {a c_k \choose n_0} (1-\psi)^{n_0} \psi ^ { a c_k - n_0 }$
	
	$P_+ = \psi (1 - d) + (1 - \psi) d$
	
	$P(n_v = x) = {a c_k \choose x} P_+^x (1-P_+)^{a c_k -x }$

	% À corriger avec sommes multiparties
	%\[P(\mbox{no other vertex activated in this cluster})= \sum_{n_0 = 1}^{a c_k} P(n_{v_c} \ge n_0)^a \left [ \sum_{x = 0}^{n_0 - 1} P(n_v = x) \right]^{l-a } \]	
	
	%\[P(\mbox{no other vertex activated in any cluster})=  \left ( \sum_{n_0 = 1}^{a c_k} P(n_{v_c} \ge n_0)^a \left [ \sum_{x = 0}^{n_0 - 1} P(n_v = x) \right]^{l-a } \right)^{c_e}\]
	
	Sans tenir compte du choix au hasard si plus sont activés.
	
	\newpage
	
	Formula for errors insted of erasures : 
	
	$P(n_{gc} = (c - c_e - 1) + \gamma + x) = {c_e \choose x} d^x (1-d)^{ce-x}$
	
	$P(n_{ge} = (c - c_e) + \gamma + x) = {c_e - 1 \choose x} d^x (1-d)^{c_e-1-x}$
	
	$P(n_{abe} = \gamma + x) = {c - 1 \choose x} d^x (1-d)^{c-1-x}$
	
	$P(n_{be} = x) = P(n_{bc} = x) = {c - 1 \choose x} d^x (1-d)^{c- 1 -x}$

	\section{Conclusion}	
	
	
	%Marche mieux pouur de petits c --> le faire sur le sparse ?
	\nocite{*}
	\bibliographystyle{plain} % Le style est mis entre crochets.
         \bibliography{bibli.bib}
\end{document}
